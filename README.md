# Wikipedia Big Data Analysis
This analysis consists of using big data tools to answer questions about datasets from Wikipedia. There are a series of analysis questions, answered using Hive and MapReduce. The tools used are determined based on the context for each question.

# Technologies
- Hadoop MapReduce
- YARN
- HDFS
- Python
- Hive
- Git + GitHub

# Features
- Find, organize, and format pageviews on any given day.
- Follow clickstreams to find relative frequencies of different pages.
- Determine relative popularity of page access methods.

# Usage
- The HQL commands can be used on similar large datasets, specifically those found in Wikipedia dumps - https://dumps.wikimedia.org/
- This script was designed to answer all sorts of questions pertaining to big data.

# Problem Statement
- 1.Which English wikipedia article got the most traffic on January 20, 2021?
