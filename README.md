# Wikipedia Big Data Analysis
This analysis consists of using big data tools to answer questions about datasets from Wikipedia. There are a series of analysis questions, answered using Hive and MapReduce. The tools used are determined based on the context for each question.

# Technologies
- Hadoop MapReduce
- YARN
- HDFS
- Python
- Hive
- Git + GitHub

# Features
- Find, organize, and format pageviews on any given day.
- Follow clickstreams to find relative frequencies of different pages.
- Determine relative popularity of page access methods.

# Usage
- The HQL commands can be used on similar large datasets, specifically those found in Wikipedia dumps - https://dumps.wikimedia.org/
- This script was designed to answer all sorts of questions pertaining to big data.

# Problem Statement
- 1.Which English wikipedia article got the most traffic on January 20, 2021?
- 2.What English wikipedia article has the largest fraction of its readers follow an internal link to another wikipedia article?
- 3.What series of wikipedia articles, starting with Hotel California, keeps the largest fraction of its readers clicking on internal links?
- 4.Find an example of an English wikipedia article that is relatively more popular in the Americas than Germany.

- Reference
- https://github.com/samye760/Wikipedia-Big-Data-Analysis
